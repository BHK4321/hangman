{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-22T17:34:47.290708Z",
     "iopub.status.busy": "2025-06-22T17:34:47.290056Z",
     "iopub.status.idle": "2025-06-22T17:34:47.403431Z",
     "shell.execute_reply": "2025-06-22T17:34:47.402888Z",
     "shell.execute_reply.started": "2025-06-22T17:34:47.290684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T19:58:31.214032Z",
     "iopub.status.busy": "2025-06-22T19:58:31.213750Z",
     "iopub.status.idle": "2025-06-22T19:58:31.265200Z",
     "shell.execute_reply": "2025-06-22T19:58:31.264460Z",
     "shell.execute_reply.started": "2025-06-22T19:58:31.214010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HangmanDataset1(Dataset):\n",
    "    def __init__(self, words, max_word_length=45, reveal_ratio=0.5):\n",
    "        self.words = [word.lower() for word in words if len(word) <= max_word_length]\n",
    "        self.max_length = max_word_length\n",
    "        self.reveal_ratio = reveal_ratio\n",
    "        self.char_to_idx = {char: i+1 for i, char in enumerate(string.ascii_lowercase)}\n",
    "        self.char_to_idx['_'] = 0  # blank\n",
    "        self.char_to_idx['PAD'] = 27\n",
    "\n",
    "    def __len__(self): return len(self.words) * 80\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx % len(self.words)]\n",
    "        reveal_count = int(len(word) * self.reveal_ratio)\n",
    "        revealed = random.sample(range(len(word)), reveal_count) if reveal_count > 0 else []\n",
    "\n",
    "        word_state = [0] * self.max_length\n",
    "        for pos in revealed: word_state[pos] = self.char_to_idx[word[pos]]\n",
    "\n",
    "        target_pos, target_chars, position_context, vowels = [], [], [0]*self.max_length, set('aeiou')\n",
    "        for i in range(len(word)):\n",
    "            if i not in revealed:\n",
    "                ctx = 0\n",
    "                if i > 0 and word_state[i-1] != 0: ctx += 1\n",
    "                if i < len(word)-1 and word_state[i+1] != 0: ctx += 2\n",
    "                if ctx:\n",
    "                    target_pos.append(i)\n",
    "                    target_chars.append(self.char_to_idx[word[i]])\n",
    "                    position_context[i] = ctx\n",
    "\n",
    "        count_blanks = word_state[:len(word)].count(0)\n",
    "        blank_vowel_next = [0]*self.max_length\n",
    "        for i in range(len(word)):\n",
    "            if word_state[i] == 0:\n",
    "                l = word[i-1] if i > 0 else 'x'\n",
    "                r = word[i+1] if i < len(word)-1 else 'x'\n",
    "                if l in vowels or r in vowels:\n",
    "                    blank_vowel_next[i] = 1\n",
    "\n",
    "        max_targets = 10\n",
    "        while len(target_pos) < max_targets:\n",
    "            target_pos.append(-1)\n",
    "            target_chars.append(0)\n",
    "\n",
    "        return {\n",
    "            'word_state': torch.tensor(word_state, dtype=torch.long),\n",
    "            'position_context': torch.tensor(position_context, dtype=torch.long),\n",
    "            'target_positions': torch.tensor(target_pos[:max_targets], dtype=torch.long),\n",
    "            'target_chars': torch.tensor(target_chars[:max_targets], dtype=torch.long),\n",
    "            'word_length': torch.tensor(len(word), dtype=torch.long),\n",
    "            'blank_count': torch.tensor(count_blanks, dtype=torch.long),\n",
    "            'next_to_vowel': torch.tensor(blank_vowel_next, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class EnhancedHangmanModel1(nn.Module):\n",
    "    def __init__(self, vocab_size=28, max_len=45, emb_dim=128, hidden_dim=1024, ablate={}):\n",
    "        super().__init__()\n",
    "        self.ablate = ablate\n",
    "        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.ctx_emb = nn.Embedding(4, 32)\n",
    "\n",
    "        self.pattern_cnn = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, 64, 3, padding=1), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Conv1d(64, 64, 3, padding=1), nn.ReLU(), nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.LSTM(emb_dim + 32, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.pos_prior_mlp = nn.Sequential(\n",
    "            nn.Linear(1 + 1 + 64, 32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32, 26)\n",
    "        )\n",
    "\n",
    "        def decoder():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_dim*2 + 26, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim // 2, 26)\n",
    "            )\n",
    "\n",
    "        self.left_decoder = decoder()\n",
    "        self.right_decoder = decoder()\n",
    "        self.both_decoder = decoder()\n",
    "\n",
    "    def forward(self, word_state, position_context, word_length, blank_count, next_to_vowel):\n",
    "        B, L = word_state.size()\n",
    "        emb = self.char_emb(word_state)\n",
    "        cnn_feat = self.pattern_cnn(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        ctx = self.ctx_emb(position_context)\n",
    "        encoded, _ = self.encoder(torch.cat([emb, ctx], -1))\n",
    "\n",
    "        pos_scores = []\n",
    "        for i in range(L):\n",
    "            is_blank = (word_state[:, i] == 0).float().unsqueeze(1)\n",
    "            bc = blank_count.unsqueeze(1).float() / L\n",
    "            pos_input = torch.cat([is_blank, bc, cnn_feat[:, i, :]], -1)\n",
    "            pos_scores.append(self.pos_prior_mlp(pos_input).unsqueeze(1))\n",
    "        priors = torch.cat(pos_scores, 1)  # [B, L, 26]\n",
    "\n",
    "        out = torch.zeros(B, L, 26, device=word_state.device)\n",
    "        for i in range(L):\n",
    "            h = encoded[:, i, :]\n",
    "            ptype = position_context[:, i]\n",
    "            inp = torch.cat([h, priors[:, i, :]], -1)\n",
    "            out[ptype==1, i, :] = self.left_decoder(inp[ptype==1])\n",
    "            out[ptype==2, i, :] = self.right_decoder(inp[ptype==2])\n",
    "            out[ptype==3, i, :] = self.both_decoder(inp[ptype==3])\n",
    "\n",
    "        return out\n",
    "\n",
    "def build_lengthwise_frequencies(word_list):\n",
    "    \"\"\"Build a map from word length to letter frequency Counter.\"\"\"\n",
    "    length_freq = defaultdict(Counter)\n",
    "    for word in word_list:\n",
    "        word = word.lower()\n",
    "        unique_letters = set(word)\n",
    "        length_freq[len(word)].update(unique_letters)\n",
    "    return length_freq\n",
    "\n",
    "def get_best_first_guess(word_length, guessed_letters, length_freq):\n",
    "    \"\"\"\n",
    "    Returns the best frequency-based first guess for a given word length,\n",
    "    excluding already guessed letters.\n",
    "    \"\"\"\n",
    "    if word_length not in length_freq:\n",
    "        # fallback to global frequency\n",
    "        total_counter = Counter()\n",
    "        for counter in length_freq.values():\n",
    "            total_counter += counter\n",
    "        freq = total_counter\n",
    "    else:\n",
    "        freq = length_freq[word_length]\n",
    "\n",
    "    sorted_letters = [letter for letter, _ in freq.most_common() if letter not in guessed_letters]\n",
    "    return sorted_letters[0] if sorted_letters else None\n",
    "\n",
    "def word_matches_pattern(word, pattern, wrong_letters):\n",
    "    \"\"\"Check if a word matches the current hangman pattern\"\"\"\n",
    "    if len(word) != len(pattern):\n",
    "        return False\n",
    "    if pattern.count('_') == len(pattern):\n",
    "        return True\n",
    "    # Reject if it contains any wrong letters\n",
    "    if any(letter in word for letter in wrong_letters):\n",
    "        return False\n",
    "    \n",
    "    # Check position-by-position for pattern match\n",
    "    for w_char, p_char in zip(word, pattern):\n",
    "        if p_char != '_' and w_char != p_char:\n",
    "            return False\n",
    "        if p_char == '_' and w_char in wrong_letters:\n",
    "            return False  # Prevent wrong letters in unknown positions\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_dictionary_filtered_multipliers(word_pattern, wrong_letters, dictionary):\n",
    "    \"\"\"\n",
    "    Filter dictionary based on current pattern and wrong letters,\n",
    "    then calculate letter frequency penalties based on positional constraints\n",
    "    around revealed substrings.\n",
    "    \"\"\"\n",
    "            \n",
    "    # Find matching words\n",
    "    matching_words = []\n",
    "    \n",
    "    pattern_str = ''.join(word_pattern)\n",
    "    for word in dictionary:\n",
    "        if word_matches_pattern(word, word_pattern, wrong_letters):\n",
    "            matching_words.append(word)\n",
    "    multipliers = {letter: 0.9 for letter in string.ascii_lowercase}\n",
    "    for letter in string.ascii_lowercase:\n",
    "        for word in matching_words:\n",
    "            for i in range(len(word)):\n",
    "                if word[i] == letter and word_pattern[i] == '_':\n",
    "                    multipliers[letter]=1.1\n",
    "                    break\n",
    "            if multipliers[letter] == 1.1:\n",
    "                break\n",
    "    return multipliers, len(matching_words), len(dictionary) - len(matching_words)\n",
    "\n",
    "def clean_state_dict(state_dict):\n",
    "    \"\"\"Removes 'module.' prefix from multi-GPU trained models if present\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "class HangmanSolver1:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = EnhancedHangmanModel1()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.dictionary = open(\"words_250000_train.txt\").read().splitlines()\n",
    "        self.char_to_idx = {c: i+1 for i, c in enumerate(string.ascii_lowercase)}\n",
    "        self.char_to_idx['_'] = 0\n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items() if v != 0}\n",
    "        self.length_freq = build_lengthwise_frequencies(self.dictionary)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.model = self.model.to(self.device)\n",
    "        # Dictionary-based RL multipliers\n",
    "        self.dict_multipliers = {letter: 1.0 for letter in string.ascii_lowercase}\n",
    "        \n",
    "        checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)\n",
    "        cleaned_checkpoint0 = clean_state_dict(checkpoint)\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            self.model.module.load_state_dict(cleaned_checkpoint0)\n",
    "        else:\n",
    "            self.model.load_state_dict(cleaned_checkpoint0)\n",
    "        self.model.eval()\n",
    "    def update_dict_multipliers(self, word_pattern, wrong_letters):\n",
    "        \"\"\"Update multipliers based on dictionary filtering\"\"\"\n",
    "        multipliers, matching_count, eliminated_count = get_dictionary_filtered_multipliers(\n",
    "            word_pattern, wrong_letters, self.dictionary\n",
    "        )\n",
    "        self.dict_multipliers = multipliers\n",
    "        return matching_count, eliminated_count\n",
    "\n",
    "    def predict_letter(self, word_state, guessed_letters=None):\n",
    "        if guessed_letters is None:\n",
    "            guessed_letters = set()\n",
    "        wrong_letters = {ch for ch in guessed_letters if ch not in word_state and ch.isalpha()}\n",
    "        if wrong_letters is None:\n",
    "            wrong_letters = set()\n",
    "        if ' ' in word_state:\n",
    "            word_state = word_state.replace(' ', '')\n",
    "        # Update dictionary-based multipliers\n",
    "        word_pattern = list(word_state)\n",
    "        matching_count, eliminated_count = self.update_dict_multipliers(word_pattern, wrong_letters)\n",
    "        # Use best first guess based on frequency\n",
    "        if word_state.count('_') == len(word_state):\n",
    "            return get_best_first_guess(len(word_state), guessed_letters, self.length_freq)\n",
    "        \n",
    "        max_length = 45\n",
    "        state_indices = []\n",
    "        position_context = []\n",
    "    \n",
    "        for i, char in enumerate(word_state):\n",
    "            if char == '_':\n",
    "                state_indices.append(0)\n",
    "                ctx = 0\n",
    "                if i > 0 and word_state[i-1] != '_': ctx += 1\n",
    "                if i < len(word_state)-1 and word_state[i+1] != '_': ctx += 2\n",
    "                position_context.append(ctx)\n",
    "            else:\n",
    "                state_indices.append(self.char_to_idx.get(char, 27))\n",
    "                position_context.append(0)\n",
    "    \n",
    "        while len(state_indices) < max_length:\n",
    "            state_indices.append(27)\n",
    "            position_context.append(0)\n",
    "    \n",
    "        word_tensor = torch.tensor([state_indices], dtype=torch.long).to(self.device)\n",
    "        context_tensor = torch.tensor([position_context], dtype=torch.long).to(self.device)\n",
    "        length_tensor = torch.tensor([len(word_state)], dtype=torch.long).to(self.device)\n",
    "        blank_count_tensor = torch.tensor([word_state.count('_')], dtype=torch.long).to(self.device)\n",
    "    \n",
    "        blank_vowel_next = [0] * max_length\n",
    "        for i in range(len(word_state)):\n",
    "            if word_state[i] == '_':\n",
    "                l = word_state[i-1] if i > 0 else 'x'\n",
    "                r = word_state[i+1] if i < len(word_state)-1 else 'x'\n",
    "                if l in 'aeiou' or r in 'aeiou':\n",
    "                    blank_vowel_next[i] = 1\n",
    "        blank_vowel_tensor = torch.tensor([blank_vowel_next], dtype=torch.float).to(self.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            model_out = self.model(\n",
    "                word_tensor, context_tensor, length_tensor,\n",
    "                blank_count_tensor, blank_vowel_tensor\n",
    "            )\n",
    "        # print(self.dict_multipliers)\n",
    "        # Apply dictionary-based multipliers\n",
    "        dict_multipliers_tensor = torch.tensor(\n",
    "            [self.dict_multipliers[chr(ord('a') + i)] for i in range(26)],\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        best_predictions = []\n",
    "        for i in range(len(word_state)):\n",
    "            if word_state[i] == '_':\n",
    "                # Apply dictionary multipliers to model predictions\n",
    "                adjusted_logits = model_out[0, i, :]\n",
    "                reveal_ratio = sum(1 for c in word_state if c != '_') / len(word_state)\n",
    "                if reveal_ratio < 0.35:\n",
    "                    adjusted_logits *= dict_multipliers_tensor\n",
    "                \n",
    "                probs = torch.softmax(adjusted_logits, dim=0)\n",
    "                for j, prob in enumerate(probs):\n",
    "                    letter = chr(ord('a') + j)\n",
    "                    if letter not in guessed_letters:\n",
    "                        best_predictions.append((letter, prob.item(), i))\n",
    "    \n",
    "        if best_predictions:\n",
    "            best_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            return best_predictions[0][0]\n",
    "    \n",
    "        # Fallback to frequency-based guess\n",
    "        available_letters = [c for c in string.ascii_lowercase if c not in guessed_letters]\n",
    "        if available_letters:\n",
    "            return available_letters[0]\n",
    "        return None\n",
    "\n",
    "def simulate_hangman_game(solver1, solver2, word, max_wrong=6, verbose=False):\n",
    "    \"\"\"Simulate a hangman game and return results\"\"\"\n",
    "    true_word = word.lower()\n",
    "    word_state = ['_'] * len(true_word)\n",
    "    guessed_letters = set()\n",
    "    wrong_letters = set()\n",
    "    wrong_count = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Word: {true_word}\")\n",
    "        print(f\"Initial state: {''.join(word_state)}\")\n",
    "    \n",
    "    while '_' in word_state and wrong_count < max_wrong:\n",
    "        # Get prediction\n",
    "        reveal_ratio = len([c for c in word_state if c != '_']) / len(true_word)\n",
    "        solver = solver1 if reveal_ratio > 0.65 else solver2\n",
    "        guess = solver.predict_letter(''.join(word_state), guessed_letters)\n",
    "        \n",
    "        if guess is None or guess in guessed_letters:\n",
    "            break\n",
    "            \n",
    "        guessed_letters.add(guess)\n",
    "        \n",
    "        # Check if guess is correct\n",
    "        if guess in true_word:\n",
    "            # Reveal all instances of the letter\n",
    "            for i, char in enumerate(true_word):\n",
    "                if char == guess:\n",
    "                    word_state[i] = char\n",
    "            if verbose:\n",
    "                print(f\"Correct guess '{guess}': {''.join(word_state)}\")\n",
    "        else:\n",
    "            wrong_letters.add(guess)\n",
    "            wrong_count += 1\n",
    "            if verbose:\n",
    "                print(f\"Wrong guess '{guess}' ({wrong_count}/{max_wrong}): {''.join(word_state)}\")\n",
    "    \n",
    "    success = '_' not in word_state\n",
    "    if verbose:\n",
    "        print(f\"Game result: {'WIN' if success else 'LOSE'}\")\n",
    "        print(f\"Final state: {''.join(word_state)}\")\n",
    "        print(f\"Wrong guesses: {sorted(wrong_letters)}\")\n",
    "    \n",
    "    return {\n",
    "        'success': success,\n",
    "        'word': true_word,\n",
    "        'guesses': len(guessed_letters),\n",
    "        'wrong_guesses': wrong_count,\n",
    "        'final_state': ''.join(word_state)\n",
    "    }\n",
    "\n",
    "def train_model1(words, epochs=10, early_stopping_patience=5):\n",
    "    print(\"Updated...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EnhancedHangmanModel1()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(opt, patience=2)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Curriculum: reveal_ratio increases with epoch (starts hard, becomes easier)\n",
    "        reveal_schedule = [0.93, 0.87, 0.80, 0.80, 0.73, 0.73, 0.67, 0.67, 0.67, 0.60, 0.60, 0.53, 0.53, 0.47, 0.47, 0.40, 0.40, 0.33, 0.33, 0.27]\n",
    "        reveal_ratio = reveal_schedule[ep] if ep < len(reveal_schedule) else 0.20\n",
    "        ds = HangmanDataset1(words, reveal_ratio=reveal_ratio)\n",
    "        train_len = int(0.9 * len(ds))\n",
    "        tr, val = random_split(ds, [train_len, len(ds)-train_len])\n",
    "        dl = DataLoader(tr, shuffle=True, pin_memory=True, batch_size=256, num_workers=4)\n",
    "        vl = DataLoader(val, pin_memory=True, batch_size=256, num_workers=4)\n",
    "\n",
    "        print(f\"\\n--- Epoch {ep+1} | Reveal Ratio: {reveal_ratio:.2f} ---\", flush=True)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(dl, desc=\"Training\", ncols=100)):\n",
    "            opt.zero_grad()\n",
    "            out = model(batch['word_state'].to(device), batch['position_context'].to(device),\n",
    "                        batch['word_length'].to(device), batch['blank_count'].to(device),\n",
    "                        batch['next_to_vowel'].to(device))\n",
    "            loss, count = 0, 0\n",
    "            for b in range(out.size(0)):\n",
    "                target_pos = batch['target_positions'][b].to(device)\n",
    "                target_char = batch['target_chars'][b].to(device)\n",
    "                for p, c in zip(target_pos, target_char):\n",
    "                    if p >= 0 and c > 0:\n",
    "                        loss += loss_fn(out[b, p], c-1)\n",
    "                        count += 1\n",
    "            if count > 0:\n",
    "                loss = loss / count\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            if i % 20 == 0:\n",
    "                if isinstance(loss, torch.Tensor):\n",
    "                    print(f\"  Batch {i}/{len(dl)} | Loss: {loss.item():.4f}\", flush=True)\n",
    "                else:\n",
    "                    print(f\"  Batch {i}/{len(dl)} | Loss: N/A (no valid targets)\", flush=True)\n",
    "\n",
    "        train_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(vl, desc=\"Validation\", ncols=100):\n",
    "                out = model(batch['word_state'].to(device), batch['position_context'].to(device),\n",
    "                            batch['word_length'].to(device), batch['blank_count'].to(device),\n",
    "                            batch['next_to_vowel'].to(device))\n",
    "                loss, count = 0, 0\n",
    "                for b in range(out.size(0)):\n",
    "                    target_pos = batch['target_positions'][b].to(device)\n",
    "                    target_char = batch['target_chars'][b].to(device)\n",
    "                    for p, c in zip(target_pos, target_char):\n",
    "                        if p >= 0 and c > 0:\n",
    "                            loss += loss_fn(out[b, p], c-1)\n",
    "                            count += 1\n",
    "                if count > 0:\n",
    "                    val_loss += loss.item() / count\n",
    "                    val_batches += 1\n",
    "\n",
    "        val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best:\n",
    "            best = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), \"best_model1.pth\")\n",
    "            print(\"âœ… Model improved and saved.\", flush=True)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"âš ï¸ No improvement. Patience: {patience_counter}/{early_stopping_patience}\", flush=True)\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"ðŸ›‘ Early stopping at epoch {ep+1}\", flush=True)\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T20:08:22.366239Z",
     "iopub.status.busy": "2025-06-22T20:08:22.365973Z",
     "iopub.status.idle": "2025-06-22T20:08:24.606120Z",
     "shell.execute_reply": "2025-06-22T20:08:24.605574Z",
     "shell.execute_reply.started": "2025-06-22T20:08:22.366217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "solver1 = HangmanSolver1(model_path= \"best_model1.pth\")\n",
    "solver2 = HangmanSolver1(model_path= \"best_model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: love\n",
      "Initial state: ____\n",
      "Wrong guess 'a' (1/6): ____\n",
      "Correct guess 'e': ___e\n",
      "Wrong guess 'r' (2/6): ___e\n",
      "Correct guess 'l': l__e\n",
      "Wrong guess 'i' (3/6): l__e\n",
      "Correct guess 'o': lo_e\n",
      "Wrong guess 'n' (4/6): lo_e\n",
      "Correct guess 'v': love\n",
      "Game result: WIN\n",
      "Final state: love\n",
      "Wrong guesses: ['a', 'i', 'n', 'r']\n"
     ]
    }
   ],
   "source": [
    "# Test on specific word\n",
    "result = simulate_hangman_game(solver1, solver2, \"love\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7709729,
     "sourceId": 12236100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7710349,
     "sourceId": 12237331,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7712710,
     "sourceId": 12240701,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7714026,
     "sourceId": 12242919,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7714904,
     "sourceId": 12244296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7715517,
     "sourceId": 12245248,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7716427,
     "sourceId": 12246585,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7717951,
     "sourceId": 12248905,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
