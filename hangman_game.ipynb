{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4862fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bb16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HangmanDataset(Dataset):\n",
    "    def __init__(self, words, max_word_length=45, reveal_ratio=0.5):\n",
    "        self.words = [word.lower() for word in words if len(word) <= max_word_length]\n",
    "        self.max_length = max_word_length\n",
    "        self.reveal_ratio = reveal_ratio\n",
    "        self.char_to_idx = {char: i+1 for i, char in enumerate(string.ascii_lowercase)}\n",
    "        self.char_to_idx['_'] = 0  # blank\n",
    "        self.char_to_idx['PAD'] = 27\n",
    "\n",
    "    def __len__(self): return len(self.words) * 80\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx % len(self.words)]\n",
    "        reveal_count = int(len(word) * self.reveal_ratio)\n",
    "        revealed = random.sample(range(len(word)), reveal_count) if reveal_count > 0 else []\n",
    "\n",
    "        word_state = [0] * self.max_length\n",
    "        for pos in revealed: word_state[pos] = self.char_to_idx[word[pos]]\n",
    "\n",
    "        target_pos, target_chars, position_context, vowels = [], [], [0]*self.max_length, set('aeiou')\n",
    "        for i in range(len(word)):\n",
    "            if i not in revealed:\n",
    "                ctx = 0\n",
    "                if i > 0 and word_state[i-1] != 0: ctx += 1\n",
    "                if i < len(word)-1 and word_state[i+1] != 0: ctx += 2\n",
    "                if ctx:\n",
    "                    target_pos.append(i)\n",
    "                    target_chars.append(self.char_to_idx[word[i]])\n",
    "                    position_context[i] = ctx\n",
    "\n",
    "        count_blanks = word_state[:len(word)].count(0)\n",
    "        blank_vowel_next = [0]*self.max_length\n",
    "        for i in range(len(word)):\n",
    "            if word_state[i] == 0:\n",
    "                l = word[i-1] if i > 0 else 'x'\n",
    "                r = word[i+1] if i < len(word)-1 else 'x'\n",
    "                if l in vowels or r in vowels:\n",
    "                    blank_vowel_next[i] = 1\n",
    "\n",
    "        max_targets = 10\n",
    "        while len(target_pos) < max_targets:\n",
    "            target_pos.append(-1)\n",
    "            target_chars.append(0)\n",
    "\n",
    "        return {\n",
    "            'word_state': torch.tensor(word_state, dtype=torch.long),\n",
    "            'position_context': torch.tensor(position_context, dtype=torch.long),\n",
    "            'target_positions': torch.tensor(target_pos[:max_targets], dtype=torch.long),\n",
    "            'target_chars': torch.tensor(target_chars[:max_targets], dtype=torch.long),\n",
    "            'word_length': torch.tensor(len(word), dtype=torch.long),\n",
    "            'blank_count': torch.tensor(count_blanks, dtype=torch.long),\n",
    "            'next_to_vowel': torch.tensor(blank_vowel_next, dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "class EnhancedHangmanModel(nn.Module):\n",
    "    def __init__(self, vocab_size=28, max_len=45, emb_dim=128, hidden_dim=1024, ablate={}):\n",
    "        super().__init__()\n",
    "        self.ablate = ablate\n",
    "        self.char_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.ctx_emb = nn.Embedding(4, 32)\n",
    "\n",
    "        self.pattern_cnn = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, 64, 3, padding=1), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Conv1d(64, 64, 3, padding=1), nn.ReLU(), nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.LSTM(emb_dim + 32, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.pos_prior_mlp = nn.Sequential(\n",
    "            nn.Linear(1 + 1 + 64, 32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32, 26)\n",
    "        )\n",
    "\n",
    "        def decoder():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_dim*2 + 26, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "\n",
    "                nn.Linear(hidden_dim // 2, 26)\n",
    "            )\n",
    "\n",
    "        self.left_decoder = decoder()\n",
    "        self.right_decoder = decoder()\n",
    "        self.both_decoder = decoder()\n",
    "\n",
    "    def forward(self, word_state, position_context, word_length, blank_count, next_to_vowel):\n",
    "        B, L = word_state.size()\n",
    "        emb = self.char_emb(word_state)\n",
    "        cnn_feat = self.pattern_cnn(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        ctx = self.ctx_emb(position_context)\n",
    "        encoded, _ = self.encoder(torch.cat([emb, ctx], -1))\n",
    "\n",
    "        pos_scores = []\n",
    "        for i in range(L):\n",
    "            is_blank = (word_state[:, i] == 0).float().unsqueeze(1)\n",
    "            bc = blank_count.unsqueeze(1).float() / L\n",
    "            pos_input = torch.cat([is_blank, bc, cnn_feat[:, i, :]], -1)\n",
    "            pos_scores.append(self.pos_prior_mlp(pos_input).unsqueeze(1))\n",
    "        priors = torch.cat(pos_scores, 1)  # [B, L, 26]\n",
    "\n",
    "        out = torch.zeros(B, L, 26, device=word_state.device)\n",
    "        for i in range(L):\n",
    "            h = encoded[:, i, :]\n",
    "            ptype = position_context[:, i]\n",
    "            inp = torch.cat([h, priors[:, i, :]], -1)\n",
    "            out[ptype==1, i, :] = self.left_decoder(inp[ptype==1])\n",
    "            out[ptype==2, i, :] = self.right_decoder(inp[ptype==2])\n",
    "            out[ptype==3, i, :] = self.both_decoder(inp[ptype==3])\n",
    "\n",
    "        return out\n",
    "\n",
    "class HangmanSolver:\n",
    "    def __init__(self, word_list, model_path=\"best_model.pth\"):\n",
    "        self.model = EnhancedHangmanModel()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=True))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        self.dictionary = word_list\n",
    "        self.char_to_idx = {c: i+1 for i, c in enumerate(string.ascii_lowercase)}\n",
    "        self.char_to_idx['_'] = 0\n",
    "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items() if v != 0}\n",
    "\n",
    "    def _fallback_prediction(self, pattern, guessed):\n",
    "        counter = Counter()\n",
    "        for word in self.dictionary:\n",
    "            if len(word) != len(pattern):\n",
    "                continue\n",
    "            match = True\n",
    "            for wc, pc in zip(word, pattern):\n",
    "                if pc != '_' and pc != wc:\n",
    "                    match = False\n",
    "                    break\n",
    "                if pc == '_' and wc in guessed:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                for i, c in enumerate(word):\n",
    "                    if pattern[i] == '_' and c not in guessed:\n",
    "                        counter[c] += 1\n",
    "\n",
    "        if not counter:\n",
    "            for c in string.ascii_lowercase:\n",
    "                if c not in guessed:\n",
    "                    return c\n",
    "            return random.choice([c for c in string.ascii_lowercase if c not in guessed])  # final fallback\n",
    "\n",
    "        for letter, _ in counter.most_common():\n",
    "            if letter not in guessed:\n",
    "                return letter\n",
    "\n",
    "        return random.choice([c for c in string.ascii_lowercase if c not in guessed])\n",
    "\n",
    "    def predict_letter(self, word_state, guessed_letters=None):\n",
    "        if guessed_letters is None:\n",
    "            guessed_letters = set()\n",
    "        if ' ' in word_state:\n",
    "            word_state = word_state.replace(' ', '')\n",
    "        if all(c == '_' for c in word_state):\n",
    "            # Return most frequent unguessed letter as the first guess\n",
    "            common_order = \"etaoinshrdlucmfwypvbgkjqxz\"\n",
    "            for letter in common_order:\n",
    "                if letter not in guessed_letters:\n",
    "                    return letter\n",
    "        word_state = word_state.lower()\n",
    "        max_length = 45\n",
    "        state_indices = []\n",
    "        position_context = []\n",
    "\n",
    "        for i, char in enumerate(word_state):\n",
    "            if char == '_':\n",
    "                state_indices.append(0)\n",
    "                context = 0\n",
    "                if i > 0 and word_state[i-1] != '_':\n",
    "                    context += 1\n",
    "                if i < len(word_state)-1 and word_state[i+1] != '_':\n",
    "                    context += 2\n",
    "                position_context.append(context)\n",
    "            else:\n",
    "                state_indices.append(self.char_to_idx.get(char, 27))\n",
    "                position_context.append(0)\n",
    "\n",
    "        while len(state_indices) < max_length:\n",
    "            state_indices.append(27)\n",
    "            position_context.append(0)\n",
    "\n",
    "        word_tensor = torch.tensor([state_indices], dtype=torch.long).to(self.device)\n",
    "        context_tensor = torch.tensor([position_context], dtype=torch.long).to(self.device)\n",
    "        length_tensor = torch.tensor([len(word_state)], dtype=torch.long).to(self.device)\n",
    "        blank_count_tensor = torch.tensor([word_state.count('_')], dtype=torch.long).to(self.device)\n",
    "        blank_vowel_next = [0]*max_length\n",
    "\n",
    "        for i in range(len(word_state)):\n",
    "            if word_state[i] == '_':\n",
    "                l = word_state[i-1] if i > 0 else 'x'\n",
    "                r = word_state[i+1] if i < len(word_state)-1 else 'x'\n",
    "                if l in 'aeiou' or r in 'aeiou':\n",
    "                    blank_vowel_next[i] = 1\n",
    "\n",
    "        blank_vowel_tensor = torch.tensor([blank_vowel_next], dtype=torch.float).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(word_tensor, context_tensor, length_tensor, blank_count_tensor, blank_vowel_tensor)\n",
    "\n",
    "        best_predictions = []\n",
    "        for i in range(len(word_state)):\n",
    "            if word_state[i] == '_' and position_context[i] > 0:\n",
    "                probs = torch.softmax(predictions[0, i, :], dim=0)\n",
    "                for j, prob in enumerate(probs):\n",
    "                    letter = chr(ord('a') + j)\n",
    "                    if letter not in guessed_letters:\n",
    "                        best_predictions.append((letter, prob.item(), i))\n",
    "\n",
    "        if not best_predictions:\n",
    "            for i in range(len(word_state)):\n",
    "                if word_state[i] == '_':\n",
    "                    probs = torch.softmax(predictions[0, i, :], dim=0)\n",
    "                    for j, prob in enumerate(probs):\n",
    "                        letter = chr(ord('a') + j)\n",
    "                        if letter not in guessed_letters:\n",
    "                            best_predictions.append((letter, prob.item(), i))\n",
    "\n",
    "        if best_predictions:\n",
    "            best_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            return best_predictions[0][0]\n",
    "        \n",
    "        # Fallback to frequency-based guess if model fails\n",
    "        common_order = \"etaoinshrdlucmfwypvbgkjqxz\"\n",
    "        for letter in common_order:\n",
    "            if letter not in guessed_letters:\n",
    "                return letter\n",
    "\n",
    "        return 'e'  # very rare fallback if all else fails\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(words, epochs=10, early_stopping_patience=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EnhancedHangmanModel()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(opt, patience=2)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # Curriculum: reveal_ratio increases with epoch (starts hard, becomes easier)\n",
    "        reveal_ratio = max(0.1, 1.0 - (ep+1) * 0.067)  # Start from 1.0, decrease to 0.1\n",
    "        ds = HangmanDataset(words, reveal_ratio=reveal_ratio)\n",
    "        train_len = int(0.9 * len(ds))\n",
    "        tr, val = random_split(ds, [train_len, len(ds)-train_len])\n",
    "        dl = DataLoader(tr, shuffle=True, pin_memory=True, batch_size=256, num_workers=4)\n",
    "        vl = DataLoader(val, pin_memory=True, batch_size=256, num_workers=4)\n",
    "\n",
    "        print(f\"\\n--- Epoch {ep+1} | Reveal Ratio: {reveal_ratio:.2f} ---\", flush=True)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(dl, desc=\"Training\", ncols=100)):\n",
    "            opt.zero_grad()\n",
    "            out = model(batch['word_state'].to(device), batch['position_context'].to(device),\n",
    "                        batch['word_length'].to(device), batch['blank_count'].to(device),\n",
    "                        batch['next_to_vowel'].to(device))\n",
    "            loss, count = 0, 0\n",
    "            for b in range(out.size(0)):\n",
    "                target_pos = batch['target_positions'][b].to(device)\n",
    "                target_char = batch['target_chars'][b].to(device)\n",
    "                for p, c in zip(target_pos, target_char):\n",
    "                    if p >= 0 and c > 0:\n",
    "                        loss += loss_fn(out[b, p], c-1)\n",
    "                        count += 1\n",
    "            if count > 0:\n",
    "                loss = loss / count\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "            if i % 20 == 0:\n",
    "                if isinstance(loss, torch.Tensor):\n",
    "                    print(f\"  Batch {i}/{len(dl)} | Loss: {loss.item():.4f}\", flush=True)\n",
    "                else:\n",
    "                    print(f\"  Batch {i}/{len(dl)} | Loss: N/A (no valid targets)\", flush=True)\n",
    "\n",
    "\n",
    "        train_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(vl, desc=\"Validation\", ncols=100):\n",
    "                out = model(batch['word_state'].to(device), batch['position_context'].to(device),\n",
    "                            batch['word_length'].to(device), batch['blank_count'].to(device),\n",
    "                            batch['next_to_vowel'].to(device))\n",
    "                loss, count = 0, 0\n",
    "                for b in range(out.size(0)):\n",
    "                    target_pos = batch['target_positions'][b].to(device)\n",
    "                    target_char = batch['target_chars'][b].to(device)\n",
    "                    for p, c in zip(target_pos, target_char):\n",
    "                        if p >= 0 and c > 0:\n",
    "                            loss += loss_fn(out[b, p], c-1)\n",
    "                            count += 1\n",
    "                if count > 0:\n",
    "                    val_loss += loss.item() / count\n",
    "                    val_batches += 1\n",
    "\n",
    "        val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if count > 0 and i % 20 == 0:\n",
    "            print(f\"  Batch {i}/{len(dl)} | Loss: {loss.item():.4f}\", flush=True)\n",
    "\n",
    "        if val_loss < best:\n",
    "            best = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), \"best_model.pth\")\n",
    "            print(\"âœ… Model improved and saved.\", flush=True)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"âš ï¸ No improvement. Patience: {patience_counter}/{early_stopping_patience}\", flush=True)\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"ðŸ›‘ Early stopping at epoch {ep+1}\", flush=True)\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3c9dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnhancedHangmanModel(\n",
       "  (char_emb): Embedding(28, 128)\n",
       "  (ctx_emb): Embedding(4, 32)\n",
       "  (pattern_cnn): Sequential(\n",
       "    (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): LSTM(160, 1024, batch_first=True, bidirectional=True)\n",
       "  (pos_prior_mlp): Sequential(\n",
       "    (0): Linear(in_features=66, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=26, bias=True)\n",
       "  )\n",
       "  (left_decoder): Sequential(\n",
       "    (0): Linear(in_features=2074, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=26, bias=True)\n",
       "  )\n",
       "  (right_decoder): Sequential(\n",
       "    (0): Linear(in_features=2074, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=26, bias=True)\n",
       "  )\n",
       "  (both_decoder): Sequential(\n",
       "    (0): Linear(in_features=2074, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = open(\"words_250000_train.txt\").read().splitlines()\n",
    "random.shuffle(dictionary)\n",
    "word_list = dictionary[:10000]\n",
    "#model = train_model(word_list, epochs=15)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = EnhancedHangmanModel()\n",
    "#torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), \"best_model.pth\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model1 = nn.DataParallel(model1)\n",
    "\n",
    "model1 = model1.to(device)\n",
    "model_path = \"best_model1.pth\"\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "\n",
    "if isinstance(model1, nn.DataParallel):\n",
    "    model1.module.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model1.load_state_dict(checkpoint)\n",
    "solver1 = HangmanSolver(word_list, model_path=\"best_model1.pth\")\n",
    "solver1.model = model1\n",
    "solver1.model.eval()\n",
    "#------------------------------------\n",
    "model2 = EnhancedHangmanModel()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model2 = nn.DataParallel(model2)\n",
    "\n",
    "model2 = model2.to(device)\n",
    "model_path0 = \"best_model2.pth\"\n",
    "checkpoint0 = torch.load(model_path0, map_location=device, weights_only=True)\n",
    "\n",
    "if isinstance(model2, nn.DataParallel):\n",
    "    model2.module.load_state_dict(checkpoint)\n",
    "else:\n",
    "    model2.load_state_dict(checkpoint0)\n",
    "solver2 = HangmanSolver(word_list, model_path=\"best_model2.pth\")\n",
    "solver2.model = model2\n",
    "solver2.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "\n",
    "\n",
    "class HangmanGame:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Hangman AI Game\")\n",
    "        self.solver1 = solver1\n",
    "        self.solver2 = solver2\n",
    "        self.word_state = []\n",
    "        self.guessed_letters = set()\n",
    "        self.attempts = 0\n",
    "        self.max_attempts = 6  # Adjusted to traditional hangman\n",
    "        self.word_length = 0\n",
    "        self.setup_screen()\n",
    "\n",
    "    def setup_screen(self):\n",
    "        self.intro_frame = tk.Frame(self.master)\n",
    "        self.intro_frame.pack(pady=20)\n",
    "        tk.Label(self.intro_frame, text=\"Enter the length of your secret word:\", font=(\"Helvetica\", 14)).pack()\n",
    "        self.length_entry = tk.Entry(self.intro_frame, font=(\"Helvetica\", 14))\n",
    "        self.length_entry.pack()\n",
    "        tk.Button(self.intro_frame, text=\"Start Game\", font=(\"Helvetica\", 14), command=self.start_game).pack(pady=10)\n",
    "\n",
    "    def start_game(self):\n",
    "        try:\n",
    "            self.word_length = int(self.length_entry.get())\n",
    "            if self.word_length <= 0:\n",
    "                raise ValueError\n",
    "        except ValueError:\n",
    "            messagebox.showerror(\"Invalid Input\", \"Please enter a valid positive integer.\")\n",
    "            return\n",
    "        self.word_state = ['_'] * self.word_length\n",
    "        self.intro_frame.destroy()\n",
    "        self.create_game_ui()\n",
    "\n",
    "    def create_game_ui(self):\n",
    "        self.canvas = tk.Canvas(self.master, width=200, height=250, bg='white')\n",
    "        self.canvas.pack(pady=10)\n",
    "        self.draw_hangman()\n",
    "\n",
    "        self.word_label = tk.Label(self.master, text=\" \".join(self.word_state), font=(\"Helvetica\", 24))\n",
    "        self.word_label.pack(pady=10)\n",
    "\n",
    "        self.info_label = tk.Label(self.master, text=\"Guessed Letters: None\\nAttempts Left: 6\", font=(\"Helvetica\", 12))\n",
    "        self.info_label.pack()\n",
    "\n",
    "        tk.Button(self.master, text=\"Let AI Guess\", font=(\"Helvetica\", 14), command=self.ai_guess).pack(pady=10)\n",
    "\n",
    "    def update_ui(self):\n",
    "        self.word_label.config(text=\" \".join(self.word_state))\n",
    "        self.info_label.config(\n",
    "            text=f\"Guessed Letters: {', '.join(sorted(self.guessed_letters)) or 'None'}\\nAttempts Left: {self.max_attempts - self.attempts}\"\n",
    "        )\n",
    "        self.draw_hangman()\n",
    "\n",
    "    def get_reveal_ratio(self):\n",
    "        total = len(self.word_state)\n",
    "        revealed = sum(1 for c in self.word_state if c != '_')\n",
    "        return revealed / total if total > 0 else 0\n",
    "\n",
    "    def ai_guess(self):\n",
    "        if '_' not in self.word_state or self.attempts >= self.max_attempts:\n",
    "            return\n",
    "        solver = self.solver1 if self.get_reveal_ratio() > 0.5 else self.solver2\n",
    "        guess = solver.predict_letter(''.join(self.word_state), self.guessed_letters)\n",
    "        self.guessed_letters.add(guess)\n",
    "        answer = messagebox.askyesno(\"AI Guess\", f\"Does the letter '{guess}' appear in your word?\")\n",
    "        if answer:\n",
    "            positions = simpledialog.askstring(\"Correct Guess\", f\"Enter 1-based positions of '{guess}' separated by commas (e.g. 1,3):\")\n",
    "            if positions:\n",
    "                try:\n",
    "                    indices = [int(i.strip()) - 1 for i in positions.split(',')]\n",
    "                    for idx in indices:\n",
    "                        if 0 <= idx < self.word_length:\n",
    "                            self.word_state[idx] = guess\n",
    "                except:\n",
    "                    messagebox.showerror(\"Error\", \"Invalid positions entered.\")\n",
    "        else:\n",
    "            self.attempts += 1\n",
    "        self.update_ui()\n",
    "        if '_' not in self.word_state:\n",
    "            messagebox.showinfo(\"Game Over\", \"ðŸŽ‰ The AI guessed your word!\")\n",
    "        elif self.attempts >= self.max_attempts:\n",
    "            messagebox.showinfo(\"Game Over\", \"ðŸ’€ The AI failed to guess your word.\")\n",
    "\n",
    "    def draw_hangman(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "        # Gallows\n",
    "        self.canvas.create_line(20, 230, 180, 230)\n",
    "        self.canvas.create_line(50, 230, 50, 20)\n",
    "        self.canvas.create_line(50, 20, 120, 20)\n",
    "        self.canvas.create_line(120, 20, 120, 40)\n",
    "\n",
    "        if self.attempts > 0:\n",
    "            self.canvas.create_oval(100, 40, 140, 80)  # Head\n",
    "        if self.attempts > 1:\n",
    "            self.canvas.create_line(120, 80, 120, 140)  # Body\n",
    "        if self.attempts > 2:\n",
    "            self.canvas.create_line(120, 100, 90, 120)  # Left arm\n",
    "        if self.attempts > 3:\n",
    "            self.canvas.create_line(120, 100, 150, 120)  # Right arm\n",
    "        if self.attempts > 4:\n",
    "            self.canvas.create_line(120, 140, 90, 180)  # Left leg\n",
    "        if self.attempts > 5:\n",
    "            self.canvas.create_line(120, 140, 150, 180)  # Right leg\n",
    "\n",
    "def run_hangman_gui():\n",
    "    root = tk.Tk()\n",
    "    game = HangmanGame(root)\n",
    "    root.mainloop()\n",
    "\n",
    "run_hangman_gui()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
